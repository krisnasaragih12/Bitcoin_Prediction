{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "# time_start is 1 January 2022\n",
    "time_start = datetime.datetime(2022, 1, 1)\n",
    "time_end = datetime.datetime.now()\n",
    "\n",
    "# Konversi waktu ke format timestamp Unix\n",
    "time_end_unix = int(time_end.timestamp())\n",
    "time_start_unix = int(time_start.timestamp())\n",
    "\n",
    "# Define the API URL\n",
    "api_url = f'https://api.coinmarketcap.com/data-api/v3/cryptocurrency/historical?id=1&convertId=2794&timeStart={time_start_unix}&timeEnd={time_end_unix}'\n",
    "\n",
    "print(api_url)\n",
    "\n",
    "r = requests.get(api_url)\n",
    "data = []\n",
    "\n",
    "# Extract data from the API response\n",
    "for item in r.json()['data']['quotes']:\n",
    "    close = item['quote']['close']\n",
    "    volume = item['quote']['volume']\n",
    "    date = item['quote']['timestamp']\n",
    "    high = item['quote']['high']\n",
    "    low = item['quote']['low']\n",
    "    open = item['quote']['open']\n",
    "    data.append([close, volume, date, high, low, open])\n",
    "\n",
    "# Define column names for the DataFrame\n",
    "cols = [\"close\", \"volume\", \"date\", \"high\", \"low\", \"open\"]\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = 'crypto_data.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f'Data saved to {csv_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyswarm import pso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from math import sqrt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('crypto_data.csv')\n",
    "\n",
    "# Convert the date column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Sort the DataFrame by date\n",
    "df.sort_values(by='date', inplace=True, ascending=True)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.title('Close Price History')\n",
    "plt.plot(df['date'], df['close'])\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price USD (IDR)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "df = df[['close', 'volume', 'high', 'low', 'open']]\n",
    "\n",
    "# Split the data into train and test\n",
    "train, test = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "train = scaler.fit_transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "# Split the data into X_train, y_train, X_test, y_test\n",
    "X_train = train[:, 1:]\n",
    "y_train = train[:, 0]\n",
    "X_test = test[:, 1:]\n",
    "y_test = test[:, 0]\n",
    "\n",
    "# Reshape the data\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define hyperparameter search space\n",
    "bounds = [(50, 150), (0.1, 0.5), (0.0001, 0.1), (25, 100), (16, 256)] # Number of units, dropout rate, learning rate, epochs, batch size\n",
    "\n",
    "# Define the cost function for PSO\n",
    "def cost_func(x, X_train, y_train, X_test, y_test):\n",
    "    # Convert particle position to hyperparameters\n",
    "    num_units = int(x[0])\n",
    "    dropout_rate = x[1]\n",
    "    learning_rate = x[2]\n",
    "    print(f'num_units: {num_units}, dropout_rate: {dropout_rate}, learning_rate: {learning_rate}, epochs: {int(x[3])}, batch_size: {int(x[4])}')\n",
    "    # Build LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(num_units, input_shape=(X_train.shape[1], X_train.shape[2]), kernel_regularizer=tf.keras.regularizers.l2(1e-4)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=int(x[3]), batch_size=int(x[4]),\n",
    "              validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr], verbose=0)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PSO\n",
    "best_params, _ = pso(cost_func, lb=np.array(bounds)[:, 0], ub=np.array(bounds)[:, 1], swarmsize=20, maxiter=20,\n",
    "                     args=(X_train, y_train, X_test, y_test))\n",
    "\n",
    "# Display results\n",
    "print('Best Hyperparameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create LSTM model\n",
    "num_units = int(best_params[0]) + 1\n",
    "dropout_rate = best_params[1]\n",
    "learning_rate = best_params[2]\n",
    "epochs = int(best_params[3]) + 1\n",
    "batch_size = int(best_params[4]) + 1\n",
    "# Create LSTM model using best hyperparameters\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_units, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Train the model using the best hyperparameters\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save the model\n",
    "model.save('model.h5')\n",
    "\n",
    "# Summary of the model\n",
    "print(model.summary())\n",
    "\n",
    "# Plot the model\n",
    "tf.keras.utils.plot_model(model, to_file='complex_model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Show the model architecture\n",
    "from IPython.display import Image\n",
    "Image(retina=True, filename='complex_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the actual vs. predicted values\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(y_test, label='Actual Close Price')\n",
    "plt.plot(y_pred, label='Predicted Close Price')\n",
    "plt.title('Actual vs. Predicted Close Price')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price USD (IDR)', fontsize=18)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = sqrt(mse)\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# load model\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert regression predictions to binary labels (1 if y_pred > threshold else 0)\n",
    "threshold = 0.5  # Adjust the threshold as needed\n",
    "y_pred_binary = (y_pred > threshold).astype(int)\n",
    "\n",
    "# Convert actual values to binary labels\n",
    "y_test_binary = (y_test > threshold).astype(int)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test_binary, y_pred_binary)\n",
    "recall = recall_score(y_test_binary, y_pred_binary)\n",
    "f1 = f1_score(y_test_binary, y_pred_binary)\n",
    "\n",
    "# Print the results\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Display classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test_binary, y_pred_binary))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
